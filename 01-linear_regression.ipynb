{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fc23c8-5bb5-4e93-8703-02d4e8b31ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All useful libraries\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "from models.linear_regression import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c49ad-0ff7-4f99-8a4e-7c2ee7ca9495",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We need to load the dataset from Human1. Deconvoled file represents that we only have the audio signal we need to work on, no background noise only the first clap and the reverb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd51c8b3-68fd-4546-a0f4-392af5fe2e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Centroid:\n",
      "(1000, 2)\n",
      "Shape of RIRs:\n",
      "(1000, 4, 667200)\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"LivingRoom_preprocessed_hack/Human1\"\n",
    "\n",
    "centroid = np.load(os.path.join(DATASET_PATH, \"centroid.npy\"))\n",
    "print(\"Shape of Centroid:\")\n",
    "print(centroid.shape)\n",
    "\n",
    "#Loading Room Impulse Response (1000 human locations x 10 microphones x M time samples)\n",
    "RIRs = np.load(os.path.join(DATASET_PATH, \"deconvoled_trim.npy\"), mmap_mode='r')\n",
    "print(\"Shape of RIRs:\")\n",
    "print(RIRs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f848c08-2719-4267-9679-0ff28efae23f",
   "metadata": {},
   "source": [
    "Compute the RMS of the first value of the first channel (mic) as exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ae0843-c4d5-4bc1-965e-15fa19319eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005250508"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_values = np.sqrt(np.mean(RIRs[0,0]**2, axis=-1))\n",
    "rms_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfd013-fbeb-4c0f-93ce-64af2dfa8c1c",
   "metadata": {},
   "source": [
    "Define the RMS function to apply to each value :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08306101-b340-48f3-acb0-a32eddff4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429bfc1e-4044-4cf1-84b7-58e027411741",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = list(map(tuple,centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73307c-cafc-4d01-a926-bdc4f76e2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "c4 = []\n",
    "# iterate through all data\n",
    "for i in range(RIRs.shape[0]):\n",
    "    c1.append(rms(RIRs[i,0]))\n",
    "    c2.append(rms(RIRs[i,1]))\n",
    "    c3.append(rms(RIRs[i,2]))\n",
    "    c4.append(rms(RIRs[i,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596875d-dd09-4711-b4fe-18f38f84d3dd",
   "metadata": {},
   "source": [
    "We now have all the RMS value for each microphone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9992bf-2360-48a8-b247-be2a7ef63459",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.array([c1,c2,c3,c4])\n",
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c16d8-b9f1-4941-a99c-54616b64e852",
   "metadata": {},
   "source": [
    "The Y are the position of the Human that we have in the centroid array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cf536-69f8-4736-8afd-8978e1546951",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all = centroid\n",
    "Y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c6c63-e70a-4426-97b1-63d823ff3cb5",
   "metadata": {},
   "source": [
    "We do not want to scale the position with each other we want to scale them to the size of the room that are provided in the research paper, we define the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3a072-6b29-40db-97a7-ba7ae4af8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(x, min, max):\n",
    "    return (x - min) / (max - min)\n",
    "\n",
    "\n",
    "def min_max_unscale(x, min, max):\n",
    "    return x * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a00d7-20d8-4bf0-bea9-eee665d4f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all[:,0] = min_max_scale(Y_all[:,0],-4000,500)\n",
    "Y_all[:,1] = min_max_scale(Y_all[:,1],-4000,2000)\n",
    "Y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95856057-aa4a-420d-9803-0473b84a1f0c",
   "metadata": {},
   "source": [
    "We normalize the channels data that are mostly 10e-5 because if we keep them like that we lose precision since the model works between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ab960-519b-4c72-b661-20d2f3eeccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = sklearn.preprocessing.normalize(X_all, axis=0)\n",
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a0248-f515-4bb7-ada6-97352ae297ad",
   "metadata": {},
   "source": [
    "Split the dataset 80% training and 20% testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805c804-289d-4af7-bcec-5f716e5ac4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = X_all.reshape(X_all.shape[1], X_all.shape[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, Y_all, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eda611-80ab-483a-93a0-cda20aa8909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7ec76-3fee-4b71-8978-1e8d78491297",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc443a-e1c5-4a76-b616-46a94bd7e96e",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bb857-ee40-4934-89e3-8a91660373a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75bb8a2-a115-4ed0-8c4c-6fdfba0d899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680acad-a997-4c35-9e7a-17fe6bffac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7dc0b-7ddf-40ba-b516-1d1f3cba3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b93da-5dc3-49f3-b658-57003dd7acb5",
   "metadata": {},
   "source": [
    "We have to convert back the position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f7a12-3474-45e6-99b9-477c0ef36174",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:,0] = min_max_unscale(y_test[:,0], -4000,500)\n",
    "y_test[:,1] = min_max_unscale(y_test[:,1], -4000,2000)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8fecb-4cee-413d-887d-a8c29a6caafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[:,0] = min_max_unscale(res[:,0],-4000,500)\n",
    "res[:,1] = min_max_unscale(res[:,1], -4000,2000)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bf975-c98f-4452-a68e-8e706773f0eb",
   "metadata": {},
   "source": [
    "Compute the Euclidean distance of each position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af3124-0672-4d15-912b-724903155a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.linalg.norm(res - y_test,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e386c723-1768-4f77-b1b0-319857a10384",
   "metadata": {},
   "source": [
    "Get the average to compute the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec77d8-c4a0-4c58-90e0-5be72d4739d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.average(dist)\n",
    "std = np.std(dist)\n",
    "print(\"Distance difference from real position in centimeters:\", avg / 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
