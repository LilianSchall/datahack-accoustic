{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio.functional as F\n",
    "import config\n",
    "\n",
    "from utils import download_dataset_if_needed\n",
    "\n",
    "sys.path.insert(0, \"models/VGGish\")\n",
    "\n",
    "from models.VGGish.vggish_multichannel import VGGishMulti\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, 'datasets/')\n",
    "\n",
    "from datasets.dataset import Dataset\n",
    "from datasets import livingroom\n",
    "from datasets.roomsetup import RoomSetup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import download_dataset_if_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ./LivingRoom_preprocessed_hack already exist, ignorign dataset downloading\n"
     ]
    }
   ],
   "source": [
    "download_dataset_if_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3343.275 , -3146.425 ,  1277.9375],\n",
       "       [-3284.5375,  1587.5   ,  1277.9375],\n",
       "       [  136.525 ,    53.975 ,  1277.9375],\n",
       "       [  -76.2   , -3594.1   ,  1277.9375]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = \"LivingRoom_preprocessed_hack/Human1\"\n",
    "\n",
    "dr = Dataset(RoomSetup(livingroom.speaker_xyz,\n",
    "                livingroom.mic_xyzs,\n",
    "                livingroom.x_min,\n",
    "                livingroom.x_max,\n",
    "                livingroom.y_min,\n",
    "                livingroom.y_max,\n",
    "                livingroom.walls), DATASET_PATH)\n",
    "livingroom.mic_xyzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Centroid:\n",
      "(1000, 2)\n",
      "Shape of RIRs:\n",
      "(1000, 4, 667200)\n"
     ]
    }
   ],
   "source": [
    "centroid = np.load(os.path.join(DATASET_PATH, \"centroid.npy\"))\n",
    "print(\"Shape of Centroid:\")\n",
    "print(centroid.shape)\n",
    "\n",
    "#Loading Room Impulse Response (1000 human locations x 10 microphones x M time samples)\n",
    "deconv = np.load(os.path.join(DATASET_PATH, \"deconvoled_trim.npy\"), mmap_mode='r')\n",
    "\n",
    "print(\"Shape of RIRs:\")\n",
    "print(deconv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'LivingRoom_preprocessed_hack', 'utils', '.gitignore', 'models', '.git', 'RoomVisualization.ipynb', '04-vgg_finetune.ipynb', 'LivingRoom_preprocessed_hack.zip', 'feature_extraction', '02-knn_training.ipynb', '01-linear_regression.ipynb']\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "save_path = \"models/VGGish/vggish_multi_model.pt\"\n",
    "error_path =\"models/VGGish/vggish_multi_errors/\"\n",
    "\n",
    "# Choose if you want to use an already saved model\n",
    "#vggish_multi = VGGishMulti(save_path=save_path, error_path=error_path)\n",
    "vggish_multi = VGGishMulti(saved_model=save_path, save_path=save_path, error_path=error_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(deconv, centroid, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VGGishMulti model\n",
    "# vggish_multi.fit(X_train, y_train, X_valid, y_valid, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1055.55404105 1415.25429605]\n"
     ]
    }
   ],
   "source": [
    "result = vggish_multi.predict(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
